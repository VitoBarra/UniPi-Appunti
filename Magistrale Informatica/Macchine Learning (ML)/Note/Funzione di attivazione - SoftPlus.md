---
Course: "[[Machine Learning (ML)]]"
tags:
  - IIA
Area: "[[Reti Neurali (NN)]]"
topic: 
SubTopic:
---
# Funzione di attivazione - SoftPlus
---
La __[[Funzioni di attivazione|funzione di attivazione]] SoftPlus__ è una approssimazione continua della funzione [[Funzione di attivazione - ReLu|ReLu]] ed è definita come :$$f(x)=\ln(1+e^x)$$![[Pasted image 20241227064938.png]]